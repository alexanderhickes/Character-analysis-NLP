{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 - 164870\n",
    "\n",
    "\n",
    "___\n",
    "word count: 3100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sussex NLTK root directory is \\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "#sys.path.append(r'C:\\Users\\alexh\\Documents\\SU\\Year2\\1st term\\NLP\\resources')\n",
    "#sys.path.append(r'/Users/davidw/Documents/teach/NLE/resources')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "import random\n",
    "import math\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import spacy\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from nltk.corpus import gutenberg\n",
    "nlp = spacy.load('en')\n",
    "from GutenbergCorpus import GutenbergCorpusReader as gcr\n",
    "#reader = gcr.GutenbergCorpusReader()\n",
    "reader = gcr.GutenbergCorpusReader(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources\\data\\gutenberg_eng')\n",
    "moby = gutenberg.raw('melville-moby_dick.txt')\n",
    "parsed_moby = nlp(moby)\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "parsed_emma = nlp(emma)\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "parsed_alice = nlp(alice)\n",
    "\n",
    "works_Nesbit = reader.get_authors_works('Nesbit, E. (Edith)')\n",
    "parsed_Railway = nlp(works_Nesbit[7][\"text\"])\n",
    "works_Defoe = reader.get_authors_works('Defoe, Daniel')\n",
    "parsed_Crusoe = nlp(works_Defoe[41]['text'])\n",
    "works_Shakespear = reader.get_authors_works('Shakespeare, William')\n",
    "parsed_Romeo = nlp(works_Shakespear[5][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It sounded an excellent plan, no doubt, and very neatly and simply arranged; the only difficulty was, that she had not the smallest idea how to set about it; and while she was peering about anxiously among the trees, a little sharp bark just over her head made her look up in a great hurry.  \n",
      "\n",
      "Why, I wouldn't say anything about it, even if I fell off the top of the house!' \n",
      "\n",
      "How am I to get in?' she repeated, aloud. ' \n",
      "\n",
      "It was much pleasanter at home,' thought poor Alice, 'when one wasn't always growing larger and smaller, and being ordered about by mice and rabbits. \n",
      "\n",
      "\"What matters it how far we go?\" his scaly friend replied.  \n",
      "\n",
      "He looked at Alice, and tried to speak, but for a minute or two sobs choked his voice. ' \n",
      "\n",
      "The only things in the kitchen that did not sneeze, were the cook, and a large cat which was sitting on the hearth and grinning from ear to ear. ' \n",
      "\n",
      "(Alice thought this must be the right way of speaking to a mouse: she had never done such a thing before, but she remembered having seen in her brother's Latin Grammar, 'A mouse--of a mouse--to a mouse--a mouse--O mouse!') \n",
      "\n",
      "I vote the young lady tells us a story.'  \n",
      "\n",
      "Certainly not!' said Alice indignantly. ' \n",
      "\n",
      "Please would you tell me,' said Alice, a little timidly, for she was not quite sure whether it was good manners for her to speak first, 'why your cat grins like that?' 'It's a Cheshire cat,' said the Duchess, 'and that's why. \n",
      "\n",
      "But the snail replied \"Too far, too far!\" and gave a look askance-- Said he thanked the whiting kindly, but he would not join the dance.  \n",
      "\n",
      "'UNimportant, of course, I meant,' the King hastily said, and went on to himself in an undertone, 'important--unimportant--unimportant--important--' as if he were trying which word sounded best.  \n",
      "\n",
      "Call it what you like,' said the Cat. ' \n",
      "\n",
      "Behead that Dormouse!  \n",
      "\n",
      "I've often seen a cat without a grin,' thought Alice; 'but a grin without a cat! \n",
      "\n",
      "He unfolded the paper as he spoke, and added 'It isn't a letter, after all: it's a set of verses.' 'Are they in the prisoner's handwriting?' asked another of the jurymen. ' \n",
      "\n",
      "And the executioner went off like an arrow.  \n",
      "\n",
      "And she squeezed herself up closer to Alice's side as she spoke.  \n",
      "\n",
      "I'm glad they've begun asking riddles.-- \n",
      "\n",
      "Hold your tongue!' added the Gryphon, before Alice could speak again.  \n",
      "\n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!  \n",
      "\n",
      "But her sister sat still just as she left her, leaning her head on her hand, watching the setting sun, and thinking of little Alice and all her wonderful Adventures, till she too began dreaming after a fashion, and this was her dream:--  \n",
      "\n",
      "A nice muddle their slates'll be in before the trial's over!' thought Alice.  \n",
      "\n",
      "The Hatter's remark seemed to have no sort of meaning in it, and yet it was certainly English. ' \n",
      "\n",
      "So she set the little creature down, and felt quite relieved to see it trot away quietly into the wood. ' \n",
      "\n",
      "Her listeners were perfectly quiet till she got to the part about her repeating 'YOU ARE OLD, FATHER WILLIAM,' to the Caterpillar, and the words all coming different, and then the Mock Turtle drew a long breath, and said 'That's very curious.' 'It's all about as curious as it can be,' said the Gryphon. ' \n",
      "\n",
      "An obstacle that came between Him, and ourselves, and it.  \n",
      "\n",
      "Consider your verdict,' he said to the jury, in a low, trembling voice. ' \n",
      "\n",
      "The Hatter shook his head mournfully. ' \n",
      "\n",
      "I wish I hadn't mentioned Dinah!' she said to herself in a melancholy tone. ' \n",
      "\n",
      "! wow!' 'Here! \n",
      "\n",
      "HEARTHRUG, NEAR THE FENDER, (WITH ALICE'S LOVE).  \n",
      "\n",
      "Alice did not quite like the look of the creature, but on the whole she thought it would be quite as safe to stay with it as to go after that savage Queen: so she waited.  \n",
      "\n",
      "Well, it's got no business there, at any rate: go and take it away!'  \n",
      "\n",
      "Presently the Rabbit came up to the door, and tried to open it; but, as the door opened inwards, and Alice's elbow was pressed hard against it, that attempt proved a failure. \n",
      "\n",
      "And then, turning to the rose-tree, she went on, 'What HAVE you been doing here?' 'May it please your Majesty,' said Two, in a very humble tone, going down on one knee as he spoke, 'we were trying--' 'I see!' said the Queen, who had meanwhile been examining the roses. ' \n",
      "\n",
      "(The jury all looked puzzled.) 'He must have imitated somebody else's hand,' said the King. \n",
      "\n",
      "They lived on treacle,' said the Dormouse, after thinking a minute or two. ' \n",
      "\n",
      "After a minute or two, they began moving about again, and Alice heard the Rabbit say, 'A barrowful will do, to begin with.' 'A barrowful of WHAT?' thought Alice; but she had not long to doubt, for the next moment a shower of little pebbles came rattling in at the window, and some of them hit her in the face. ' \n",
      "\n",
      "On this the White Rabbit blew three blasts on the trumpet, and then unrolled the parchment scroll, and read as follows:-- 'The Queen of Hearts, she made some tarts, All on a summer day: The Knave of Hearts, he stole those tarts, And took them quite away!' 'Consider your verdict,' the King said to the jury. ' \n",
      "\n",
      "Alice began to feel very uneasy: to be sure, she had not as yet had any dispute with the Queen, but she knew that it might happen any minute, 'and then,' thought she, 'what would become of me? \n",
      "\n",
      "There was a sound of many footsteps, and Alice looked round, eager to see the Queen.  \n",
      "\n",
      "She went on growing, and growing, and very soon had to kneel down on the floor: in another minute there was not even room for this, and she tried the effect of lying down with one elbow against the door, and the other arm curled round her head.  \n",
      "\n",
      "This is the driest thing I know. \n",
      "\n",
      "that's about the right distance--but then I wonder what Latitude or Longitude I've got to?' \n",
      "\n",
      "Digging for apples, yer honour!' ' \n",
      "\n",
      "Beau--ootiful Soo--oop \n",
      "\n",
      "The long grass rustled at her feet as the White Rabbit hurried by--the frightened Mouse splashed his way through the neighbouring pool--she could hear the rattle of the teacups as the March Hare and his friends shared their never-ending meal, and the shrill voice of the Queen ordering off her unfortunate guests to execution--once more the pig-baby was sneezing on the Duchess's knee, while plates and dishes crashed around it--once more the shriek of the Gryphon, the squeaking of the Lizard's slate-pencil, and the choking of the suppressed guinea-pigs, filled the air, mixed up with the distant sobs of the miserable Mock Turtle.  \n",
      "\n",
      "And washing?' said the Mock Turtle. ' \n",
      "\n",
      "'Is that the way YOU manage?' Alice asked.  \n",
      "\n",
      "Alice felt dreadfully puzzled. \n",
      "\n",
      "They had not gone far before they saw the Mock Turtle in the distance, sitting sad and lonely on a little ledge of rock, and, as they came nearer, Alice could hear him sighing as if his heart would break. \n",
      "\n",
      "Let me see--how IS it to be managed? \n",
      "\n",
      "This was such a new idea to Alice, that she was quite silent for a minute or two, which gave the Pigeon the opportunity of adding, 'You're looking for eggs, I know THAT well enough; and what does it matter to me whether you're a little girl or a serpent?' 'It matters a good deal to ME,' said Alice hastily; 'but I'm not looking for eggs, as it happens; and if I was, I shouldn't want YOURS: I don't like them raw.' ' \n",
      "\n",
      "This piece of rudeness was more than Alice could bear: she got up in great disgust, and walked off; the Dormouse fell asleep instantly, and neither of the others took the least notice of her going, though she looked back once or twice, half hoping that they would call after her: the last time she saw them, they were trying to put the Dormouse into the teapot. ' \n",
      "\n",
      "Well, I never heard it before,' said the Mock Turtle; 'but it sounds uncommon nonsense.'  \n",
      "\n",
      "So she began: 'O Mouse, do you know the way out of this pool? \n",
      "\n",
      "Now at OURS they had at the end of the bill, \"French, music, AND WASHING--extra.\"' 'You couldn't have wanted it much,' said Alice; 'living at the bottom of the sea.' 'I couldn't afford to learn it.' \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice called after it; and the others all joined in chorus, 'Yes, please do!' but the Mouse only shook its head impatiently, and walked a little quicker. 'What a pity it wouldn't stay!' sighed the Lory, as soon as it was quite out of sight; and an old Crab took the opportunity of saying to her daughter 'Ah, my dear! \n",
      "\n",
      "I suppose Dinah'll be sending me on messages next!' \n",
      "\n",
      "I'm never sure what I'm going to be, from one minute to another! \n",
      "\n",
      "A cat may look at a king,' said Alice. ' \n",
      "\n",
      "Ah! that accounts for it,' said the Hatter. ' \n",
      "\n",
      "Indeed, she had quite a long argument with the Lory, who at last turned sulky, and would only say, 'I am older than you, and must know better'; and this Alice would not allow without knowing how old it was, and, as the Lory positively refused to tell its age, there was no more to be said.  \n",
      "\n",
      "This did not seem to encourage the witness at all: he kept shifting from one foot to the other, looking uneasily at the Queen, and in his confusion he bit a large piece out of his teacup instead of the bread-and-butter.  \n",
      "\n",
      "However, she got up, and began to repeat it, but her head was so full of the Lobster Quadrille, that she hardly knew what she was saying, and the words came very queer indeed:-- ''Tis the voice of the Lobster; I heard him declare, \"You have baked me too brown, I must sugar my hair.\"  \n",
      "\n",
      "What WILL become of me?'  \n",
      "\n",
      "I suppose you'll be telling me next that you never tasted an egg!' 'I HAVE tasted eggs, certainly,' said Alice, who was a very truthful child; 'but little girls eat eggs quite as much as serpents do, you know.' 'I don't believe it,' said the Pigeon; 'but if they do, why then they're a kind of serpent, that's all I can say.'  \n",
      "\n",
      "Well, I'd hardly finished the first verse,' said the Hatter, 'when the Queen jumped up and bawled out, \"He's murdering the time! \n",
      "\n",
      "But, when the tide rises and sharks are around, His voice has a timid and tremulous sound.] 'That's different from what I used to say when I was a child,' said the Gryphon. ' \n",
      "\n",
      "said the Dormouse, who was sitting next to her. ' \n",
      "\n",
      "If I or she should chance to be Involved in this affair, He trusts to you to set them free,  \n",
      "\n",
      "Yes, I think you'd better leave off,' said the Gryphon: and Alice was only too glad to do so. ' \n",
      "\n",
      "The executioner's argument was, that you couldn't cut off a head unless there was a body to cut it off from: that he had never had to do such a thing before, and he wasn't going to begin at HIS time of life.  \n",
      "\n",
      "Quick, now!' \n",
      "\n",
      "said the March Hare. ' \n",
      "\n",
      "Down, down, down. \n",
      "\n",
      "Who ever saw one that size? \n",
      "\n",
      "An enormous puppy was looking down at her with large round eyes, and feebly stretching out one paw, trying to touch her. ' \n",
      "\n",
      "At this the whole pack rose up into the air, and came flying down upon her: she gave a little scream, half of fright and half of anger, and tried to beat them off, and found herself lying on the bank, with her head in the lap of her sister, who was gently brushing away some dead leaves that had fluttered down from the trees upon her face. 'Wake up, Alice dear!' said her sister; 'Why, what a long sleep you've had!' ' \n",
      "\n",
      "Last came a little feeble, squeaking voice, ('That's Bill,' thought Alice,) 'Well, I hardly know--No more, thank ye; I'm better now--but I'm a deal too flustered to tell you--all I know is, something comes at me like a Jack-in-the-box, and up I goes like a sky-rocket!' ' \n",
      "\n",
      "I thought you did,' said the Mouse. ' \n",
      "\n",
      "The Cat only grinned when it saw Alice. \n",
      "\n",
      "Would you tell me, please, which way I ought to go from here?' 'That depends a good deal on where you want to get to,' said the Cat. 'I don't much care where--' said Alice. ' \n",
      "\n",
      "said the Queen. ' \n",
      "\n",
      "I've a right to think,' said Alice sharply, for she was beginning to feel a little worried. ' \n",
      "\n",
      "Poor little thing!' said Alice, in a coaxing tone, and she tried hard to whistle to it; but she was terribly frightened all the time at the thought that it might be hungry, in which case it would be very likely to eat her up in spite of all her coaxing.  \n",
      "\n",
      "(It was this last remark that had made the whole party look so grave and anxious.)  \n",
      "\n",
      "Alice could think of nothing else to say but 'It belongs to the Duchess: you'd better ask HER about it.' 'She's in prison,' the Queen said to the executioner: 'fetch her here.'  \n",
      "\n",
      "While she was trying to fix on one, the cook took the cauldron of soup off the fire, and at once set to work throwing everything within her reach at the Duchess and the baby--the fire-irons came first; then followed a shower of saucepans, plates, and dishes. \n",
      "\n",
      "Come on, then!' roared the Queen, and Alice joined the procession, wondering very much what would happen next. ' \n",
      "\n",
      "This answer so confused poor Alice, that she let the Dormouse go on for some time without interrupting it. 'They were learning to draw,' the Dormouse went on, yawning and rubbing its eyes, for it was getting very sleepy; 'and they drew all manner of things--everything that begins with an M--' 'Why with an M?' said Alice. ' \n",
      "\n",
      "Pinch him! \n",
      "\n",
      "Take your choice!'  \n",
      "\n",
      "Call the next witness!' \n",
      "\n",
      "I'm sure _I_ shan't be able \n",
      "\n",
      "They must go by the carrier,' she thought; 'and how funny it'll seem, sending presents to one's own feet! \n",
      "\n",
      "And in she went.  \n",
      "\n",
      "'There's no such thing!' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed(164870) # replace '12345678' with your candidate number \n",
    "\n",
    "sample_size = 100\n",
    "my_sample = random.sample(list(parsed_alice.sents),sample_size) # select a random sample of sentences\n",
    "for sent in my_sample:\n",
    "    sent = re.sub(\"\\s+\",\" \",sent.text) # clean up the whitespace\n",
    "    print(sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a random sample of 100 sentences taken from 'Alice in Wonderland'. Firstly, I have used displaCy's Named Entity Visualizer to assess spaCy's performance by looking at sentences containing named entities. This is a qualitative evaluation. I have then used the method 'get_entity_counts' to provide a more quantitative analysis. This is done using illustrative examples below and 2 confusion matrix for error logging.\n",
    "I have provided 10 select sentences, and tried to analyse thoroughly via DisplaCy.\n",
    "##### sentence 4:\n",
    "'Alice' correctly identified as a PERSON.\n",
    "##### sentence 6:\n",
    "'Alice' correctly identified as a PERSON. GPE is incorrect. No mention of countries, cities or states.\n",
    "##### sentence 8:\n",
    "'Latin Grammar' incorrectly identified as a PERSON. You can see how this mistake has occurred via a dependency with 'brothers'. Having 'Latin Grammar' in capitals may also mislead spaCy into believing it is a name. \n",
    "##### sentence 11:\n",
    "'Cheshire' ambiguously labelled as a GPE, which is correct, as Cheshire is a county/state. However, in this context it is referencing a (fictional) character, the cat. I am marking this as incorrect in regards to the context of the sentence. It is not clear why Carroll gave the cat this name, and no descriptions directly link to the county of Cheshire.\n",
    "'Duchess' has been incorrectly identified as a PRODUCT. This is incorrect. The 'Duchess' is a PERSON.\n",
    "##### sentence 13:\n",
    "'King' has not been identified. Should be a PERSON. Sentence labelled incorrectly as GPE. No reference of countries, cities or states.\n",
    "##### sentence 14:\n",
    "Sentence labelled incorrectly as GPE. No reference of countries, cities or states. 'Cat' could be labelled as a PERSON. \n",
    "##### sentence 23:\n",
    "'Adventures' incorrectly labelled as a PERSON. \n",
    "##### sentence 25:\n",
    "'Hatter' incorrectly identified as an ORG, should be a PERSON. 'English' should be a LANGUAGE.\n",
    "##### sentence 27:\n",
    "'OLD' incorrectly identified as an ORG. 'WILLIAM' correctly identified as a PERSON ('FARTHER WILLIAM' would have been more accurate). 'Caterpillar' incorrectly identified as an ORG, should be a PERSON. Until now, 'Gryphon' has not been identified. Here it is incorrectly identified as an ORG, should be a PERSON.\n",
    "##### sentence 37:\n",
    "'May' incorrectly identified as a DATE, simple a manner of speech. 'Majesty' incorrectly identified as a PRODUCT, should be a PERSON.\n",
    "The confusion matrix below accounts for sentences up to 37. PERSON being labelled as either ORG or GPE is likely down to the fictional representation of characters in Alice in wonderland (E.g - Hatter, Gryphon, Dormouse, and Duchess). Entities containing capitals also provided incorrect labelling of PERSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `PERSON` | `NORP` | `FACILITY` | `ORG` | `GPE` | `LOC` | `PRODUCT` | `EVENT` | `WORK_OF_ART` | `LAW` | `DATE` | `PERCENT` |  `TIME` | `MONEY` | `QUANTITY` | `ORDINAL` | `CARDINAL`\n",
    ":----: |---\n",
    "`PERSON` | | | |5|2| |3| | | | | | | | | | \n",
    "`NORP` | | | | | | | | | | | | | | | | | \n",
    "`FACILITY` | | | | | | | | | | | | | | | | | \n",
    "`ORG` | | | | | | | | | | | | | | | | | \n",
    "`GPE` | | | | | | | | | | | | | | | | | \n",
    "`LOC` | | | | | | | | | | | | | | | | | \n",
    "`PRODUCT` | | | | | | | | | | | | | | | | | \n",
    "`EVENT` | | | | | | | | | | | | | | | | | \n",
    "`WORK_OF_ART` | | | | | | | | | | | | | | | | | \n",
    "`LAW` | | | | | | | | | | | | | | | | | \n",
    "`LANGUAGE` |2| | | | | | | | | | | | | | | | \n",
    "`DATE` | | | | | | | | | | | | | | | | | \n",
    "`TIME` | | | |1| | | | | | | | | | | | | \n",
    "`PERCENT` | | | | | | | | | | | | | | | | | \n",
    "`MONEY` | | | | | | | | | | | | | | | | | \n",
    "`QUANTITY` | | | | | | | | | | | | | | | | | \n",
    "`ORDINAL` | | | | | | | | | | | | | | | | | \n",
    "`CARDINAL` | | | | | | | | | | | | | | | | | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass 'my_sample' into method 'named_entity_counts, we get a list of entities labelled to 'entity_type'. We can manipulate the entity type to see what entities are being incorrectly listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Mention</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gryphon</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Queen</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mock</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mouse</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rabbit</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bill</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mock Turtle</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dinah</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dormouse</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Entity Mention  Count\n",
       "0          Alice    387\n",
       "1        Gryphon     43\n",
       "2          Queen     40\n",
       "3           Mock     23\n",
       "4          Mouse     22\n",
       "5         Rabbit     19\n",
       "6           Bill     14\n",
       "7    Mock Turtle     12\n",
       "8          Dinah     10\n",
       "9       Dormouse      9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def named_entity_counts(document,named_entity_label):\n",
    "    occurrences = [ent.string.strip() for ent in document.ents \n",
    "                   if ent.label_ == named_entity_label and ent.string.strip()]\n",
    "    return Counter(occurrences)\n",
    "\n",
    "text = parsed_alice\n",
    "entity_type = 'PERSON'\n",
    "#entity_type = 'GPE'\n",
    "#entity_type = 'ORG'\n",
    "#entity_type = 'PRODUCT'\n",
    "#entity_type = 'WORK_OF_ART'\n",
    "#entity_type = 'EVENT'\n",
    "#entity_type = 'NORP'\n",
    "#entity_type = 'ORDINAL'\n",
    "#entity_type = 'LANGUAGE'\n",
    "#entity_type = 'LAW'\n",
    "#entity_type = 'LOC'\n",
    "#entity_type = 'FACILITY'\n",
    "number_of_entities = 10\n",
    "display(pd.DataFrame(named_entity_counts(text,entity_type).most_common(number_of_entities),columns=[\"Entity Mention\",\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take 'PERSON', 'Alice' is the most mention character by almost 10x: Alice is very common name for a female. However, the second most common is 'Gryphon'. From DisplaCy, we know that spaCy made a few errors on the classification of 'Gryphon', stating it was an ORG (sentence 27). This example explains a lot as we see that there are a good selection of main characters stated, but not in high concentrations (compare to 'Alice'). This is due to many entities being labelled incorrectly (on analysis of label: 'GPE', Queen is most common by 23 occurrences. This means Queen has been labelled incorrectly 23 times as a 'GPE'. Below, I have created another confusion matrix. Here I have tested top the 10 most common entities from all types, hoping to provide far more quantitative analysis.\n",
    " \n",
    "'PERSON' gave the highest accuracy. Although half the names are animals, it has not included any entities that would not be considered a character in 'Alice In Wonderland'.\n",
    " \n",
    "'GPE' labelled 'Queen'(23), 'Gryphon'(5), 'Alice'(3), 'ME'(3), 'Turtle'(3) and 'Eaglet'(2) incorrectly. Correctly mentioned 'Wonderland'(3), 'Paris'(2) and 'Rome'(2). It classified 'Cheshire' as a 'GPE'. Technically correct, but not in the context of the book. A solution would be to use the opinion extractor from Assignment 1 to attach the NOUN 'Cat' in front of Cheshire. This might alter how 'Cheshire' is interpreted by spaCy.\n",
    "\n",
    "'ORG' returned interesting values. As 'Alice in Wonderland' is a fiction novel, it does not contain official 'ORG'(organisations). However, characters, especially ones named by its species, have provided similar levels of error. Maybe a bias personal observation, but the labels given seem to sound like pub names. Having 'the' at the beginning does give an impression of an establishment. 'the White Rabbit' is a typical pub name.\n",
    "\n",
    "'PRODUCT' received the lowest count. 'Cat'(3) being the most common, followed by four other incorrectly labelled entities.\n",
    "\n",
    "'FACILITY' returned nothing.\n",
    "\n",
    "The other labels produced still produced errors, but minor enough to call anomalies. Below I have included a second confusion matrix based on data obtained from the entity extractor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| `PERSON` | `NORP` | `FACILITY` | `ORG` | `GPE` | `LOC` | `PRODUCT` | `EVENT` | `WORK_OF_ART` | `LAW` | `DATE` | `PERCENT` |  `TIME` | `MONEY` | `QUANTITY` | `ORDINAL` | `CARDINAL`\n",
    ":----: |---\n",
    "`PERSON` | | | |84|39| |8| | | | | | | | |1| \n",
    "`NORP` | | | | | | | | | | | | | | | | | \n",
    "`FACILITY` | | | | | | | | | | | | | | | | | \n",
    "`ORG` | | | | | | | | | | | | | | | | | \n",
    "`GPE` | | | | | | | | | |2| | | | | | | \n",
    "`LOC` | | | | | | | | | | | | | | | | | \n",
    "`PRODUCT` | | | | | | | | | | | | | | | | | \n",
    "`EVENT` | | | | | | | | | | | | | | | | | \n",
    "`WORK_OF_ART` | | | | | | | | | | | | | | | | | \n",
    "`LAW` | | | | | | | | | | | | | | | | | \n",
    "`LANGUAGE` | | | | | | | | |9| | | | | | | | \n",
    "`DATE` | | | | | | | | | | | | | | | | | \n",
    "`TIME` | | | | | | | | | | | | | | | | | \n",
    "`PERCENT` | | | | | | | | | | | | | | | | | \n",
    "`MONEY` | | | | | | | | | | | | | | | | | \n",
    "`QUANTITY` | | | | | | | | | | | | | | | | | \n",
    "`ORDINAL` | | | | | | | | | | | | | | | | | \n",
    "`CARDINAL` | | | | | | | | | | | | | | | | | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more quantitative analysis has enforced the initial claims. As most novels, the focus is the character, so it would make sense for a book like 'Alice in Wonderland' to exceed in labelling people. However, due to the challenge of said novel, it would be very hard for someone to classify these sorts of animal names as characters. It is valid to say animals are not people, and so adding to the challenge of defining what a 'PERSON' is. We know that 'Alice in Wonderland' is based on real people: L.Carroll being the 'Dodo', the 'Hatter' representing Theophilus Carter, etc. This is also famously the case in George Orwells 'Animal Farm', where historical figures are portrayed as animals. \n",
    "\n",
    "My conclusion on spaCy's performance on extracting entities has been impressive, given the ambiguity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method for creating gender map.\n",
    "#Works by initialising a Dictionary. Takes file \"names.csv\" and adds these too dictionary called 'names_info. \n",
    "#This is then transfered to 'gender_map' dictionary, where every name is paired to its most frequent name, otherwise, unknown.\n",
    "#Param: csv.DictReader\n",
    "#Return: gender_map\n",
    "def create_gender_map(dict_reader):\n",
    "    names_info = defaultdict(lambda: {\"gender\":\"\", \"freq\": 0.0})     #initialise dictionary.\n",
    "    for row in input_file:\n",
    "        name = row[\"name\"].lower()\n",
    "        if names_info[name][\"freq\"] < float(row[\"freq\"]):            # is this gender more frequent?\n",
    "            names_info[name][\"gender\"] = row[\"gender\"] \n",
    "            names_info[name][\"freq\"] = float(row[\"freq\"])\n",
    "    gender_map = defaultdict(lambda: \"unknown\")\n",
    "    for name in names_info:\n",
    "        gender_map[name] = names_info[name][\"gender\"]\n",
    "    return gender_map\n",
    "\n",
    "input_file = csv.DictReader(open(\"names.csv\"))\n",
    "gender_map = create_gender_map(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emma</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harriet</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weston</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elton</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knightley</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Woodhouse</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jane</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Frank Churchill</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Miss Woodhouse</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jane Fairfax</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name   Gender\n",
       "0             Emma   female\n",
       "1          Harriet   female\n",
       "2           Weston     male\n",
       "3            Elton     male\n",
       "4        Knightley  unknown\n",
       "5        Woodhouse  unknown\n",
       "6             Jane   female\n",
       "7  Frank Churchill     male\n",
       "8   Miss Woodhouse   female\n",
       "9     Jane Fairfax   female"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Method to return the gender of a given name, otherwise unknown.\n",
    "#Param:  name. \n",
    "#        gender_map.\n",
    "#Return: 'male', 'female' or 'unknown'.\n",
    "def guess_gender(name, gender_map):\n",
    "    if gender_map[name] == 'unknown':                                                       #If name is not known\n",
    "        if ' ' in name:                                                                     #and contains a sapce,\n",
    "            gender_map[name.split(' ', 1)[1].strip()] = gender_map[name.split(' ', 1)[0]]   #Search 'gender_map' via first name, and add lastname to 'gender_map' with equivilent gender.\n",
    "        return gender_map[name.split(' ', 1)[0].strip()]                                    #return gender\n",
    "    return gender_map[name]                     \n",
    "\n",
    "#Method to extend the gender_map.\n",
    "#I have provided common titles, with equivilent gender notation.\n",
    "#Param:  gender_map.\n",
    "#Return: gender_map - updated.\n",
    "def extend_gender(gender_map):      \n",
    "    gender_map[\"mr\"] = 'male' \n",
    "    gender_map[\"master\"] = 'male'\n",
    "    gender_map[\"mrs\"] = 'female'\n",
    "    gender_map[\"miss\"] = 'female'\n",
    "    gender_map[\"duchess\"] = 'female'\n",
    "    gender_map[\"lord\"] = 'male'\n",
    "    return gender_map\n",
    "\n",
    "#Method to return a counter of a particular entity in a book.\n",
    "#Add entity if label is 'PERSON' and string is not empty.\n",
    "#Param:  document - Where text is held.\n",
    "#        named_entity_label - set to: 'PERSON'.\n",
    "#Return: Counter of occurrences of an entity.\n",
    "def named_entity_counts(document,named_entity_label):\n",
    "    occurrences = [ent.string.strip() for ent in document.ents\n",
    "                   if ent.label_ == named_entity_label and ent.string.strip()]\n",
    "    return Counter(occurrences)\n",
    "\n",
    "text = parsed_emma        #Take our novel.\n",
    "entity_type = 'PERSON'    #Declare type of entity we are looking for.\n",
    "number_of_entities = 10  #State how many entities we want.\n",
    "extend_gender(gender_map) #Extend current gender_map.\n",
    "names_with_gender = [(name,guess_gender(name.lower(), gender_map)) for name,count in named_entity_counts(text,entity_type).most_common(number_of_entities)]   #Create a tuple for the name and its gender as found in gender_map. do this for all names generated in 'occurrences'. \n",
    "#names_with_gender = [(name,guess_gender(name.lower(), gender_map)) for name,count in named_entity_counts(text,entity_type).most_common(number_of_entities)]\n",
    "display(pd.DataFrame(names_with_gender,columns=[\"Name\",\"Gender\"]))    #Display results in table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the original example from topic 7 using 'parsed_emma', there were 5 unknown results. 4('Knightley') and 5('Woodhouse') returned 'unknown'. This is because 'names.csv' does not contain these names, so cannot return information on their gender. A simple solution to this would be to add these names into the document with their correct gender frequencies.\n",
    "\n",
    "However, 7, 8 and 9 also returned 'unknown'. This was due to passing 2 strings: a forename ('Frank') or title ('Miss') accompanied with a surname ('Churchill'). 'names.csv' only considers forenames (and 'MISS', but not 'MR' or 'MRS') so the surname must be exempt.\n",
    "\n",
    "Firstly, I create a 'gender_map’, which takes 'names.csv' and creates a dictionary mapping a forename to its most likely gender. I have created a function 'guess_gender’, which, if the given name returns 'unknown', will find a space and remove the latter string, and use this to search the 'gender_map'. Take example 'Frank Churchill'. 'Frank Churchill' is not contained in 'names.csv' so will return 'unknown'. However, if we remove 'Churchill' and try 'Frank' instead, it returns 'male'. This is the same for 'Jane Fairfax'. \n",
    "\n",
    "If we take example 'Miss Woodhouse' (where 'Woodhouse' alone would return 'unknown' due to being a uncommon name) we get 'female'. On examining 'names.csv', you will find 'MISS' contained as a name, with gender 'female' attached. However, other titles like 'Mr' and 'Mrs' are not included (titles like 'KING' and 'QUEEN' are, which are particular useful in analysing books such as 'Alice in Wonderland'). \n",
    "\n",
    "To solve this, I have created a function 'extend_gender’, which adds these keys to the 'gender_map' with their appropriate gender values. This means, although missing from 'names.csv', we can add specific keys with appropriate gender properties. \n",
    "\n",
    "We still have two unknown values being returned: 'Knightley' and 'Woodhouse'. This can get complicated, as, although the main protagonist is 'Emma Woodhouse', there is also her farther 'Henry Woodhouse'. Woodhouse alone cannot be used to specify a gender, nor would it be used without a forename for context. Therefore, the 'unknown' result for 'Knightley' and 'Woodhouse' is down to inaccuracy in the named entity parser used by spaCy. An accurate named entity parser would provide either 'Mr/Henry/Miss/Emma Woodhouse' and 'Mr/George/John Knightley'. All of these first names/titles can be found in 'gender_map' after applying 'extend_gender'.\n",
    "\n",
    "For the bonus section, it would be possible to identify the gender of 'Knightley' as both 'John' and 'George' are male. However, this is not the case with 'Woodhouse', as this could be referencing either 'Emma' or 'Henry'.\n",
    "\n",
    "One feature I was impressed with was the success of the bonus section asking the classify the gender of a person when only referenced by their surname. A good example is 'Fairfax'. Testing 'Fairfax' alone provided 'unknown'. However, by processing 'Jane Fairfax', I was able to store 'fairfax' in the 'gender_map' as that of 'Jane'. By taking the first string, assessing the gender, then saving the last name to have the gender as the first, this provided successful results that would change when the name/title was changed.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my code to explore characters portrayed by a novelist. It allows a user to specify which novels it wants to analyse (this means more date, useful for gender analysis later), how many main characters to explore, number of common verbs to remove, dependencies to target and most importantly, which genders to consider. From here, the program will go through each entity of each novel, trying to get any interesting context. If something of interest is found, counter dictionary context will append the entity (a main character) and its associated verb as its feature set.  \n",
    "\n",
    "Results are shown in a dataframe containing main characters as columns and features as the row. A character is known to express levels of a feature based on the value provided. This value is a count of the times the character is associated to the feature. I have also created sets for when pronouns are referenced. This is useful for comparing male and female features, as it provides more data (actual names are irrelevant for gender analysis, all this is important is to distinguish a particular feature belongs to either a male or female). \n",
    "\n",
    "Further analysis in comments and below code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method for getting 'PERSON' entities.\n",
    "#Param:  parsed_novel.\n",
    "#        entity_type - 'PERSON'.\n",
    "#Return: list of 'PERSON' entities.\n",
    "def get_entities_in(parsed_novel,entity_type):\n",
    "    return [ent.text.strip().lower() for ent in parsed_novel.ents \n",
    "            if ent.label_ == entity_type and ent.text.strip()]\n",
    "\n",
    "#Method for getting the main characters from a novel using 'get_entities_in' method.\n",
    "#Param:  parsed_novel.\n",
    "#        num_characters.\n",
    "#Return: list of main characters in the novel. \n",
    "def get_main_characters(parsed_novel,num_characters):\n",
    "    get_all_characters = get_entities_in(parsed_novel, 'PERSON')\n",
    "    return [character for character, count in Counter(get_all_characters).most_common(num_characters)]\n",
    "\n",
    "#Method for getting gendered pronouns.\n",
    "#If a noun chunk contains pronoun when stripped, append the pronoun and its root verb to nounphrases.\n",
    "#Param:  parsed_novel.\n",
    "#         nounphrases.\n",
    "def get_gendered_pronoun(parsed_novel, nounphrases):\n",
    "    \n",
    "    #Method for extracting female pronoun features.\n",
    "    def gendered_pronoun_female(np):\n",
    "        return np.text.strip() in [\"she\", \"her\"]    \n",
    "    #Method for extracting male pronoun features.\n",
    "    def gendered_pronoun_male(np):\n",
    "        return np.text.strip() in [\"he\", \"his\"]\n",
    "    #Method for getting nounphrase from noun chunks within parsed novel.\n",
    "    #Param:  np.\n",
    "    #return: boolean. \n",
    "    def gendered_pronoun(np):\n",
    "        return np.text.strip() in [\"he\", \"she\", \"her\", \"his\"]\n",
    "\n",
    "    text = parsed_novel     \n",
    "    nounphrases += [[re.sub(\"\\s+\",\" \",np.text), np.root.head.text] for np in parsed_novel.noun_chunks if gendered_pronoun(np)]    \n",
    "    return nounphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atkins</th>\n",
       "      <th>jim</th>\n",
       "      <th>peter</th>\n",
       "      <th>phil</th>\n",
       "      <th>porter</th>\n",
       "      <th>romeo</th>\n",
       "      <th>will atkins</th>\n",
       "      <th>william atkins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>begin</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>behave</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bring</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculate</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carry</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clap</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condescend</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confess</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cringe</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cry</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decide</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>determine</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>die</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doubt</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drink</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drop</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dump</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclude</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fall</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seem</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shake</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shout</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sin</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sit</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sniff</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sow</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spring</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stand</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suggest</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teach</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tear</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tell</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thump</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tramp</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>try</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfasten</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wake</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wheel</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whisper</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wound</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wring</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           atkins jim peter phil porter romeo will atkins william atkins\n",
       "add             1         1                                             \n",
       "answer                    2                                             \n",
       "ask                 1    12                                             \n",
       "be              1   5    15    1      2     3           1               \n",
       "begin           1         1                             1               \n",
       "behave                                                                 1\n",
       "bring                     1                                             \n",
       "calculate                 1                                             \n",
       "call                      1                             2              1\n",
       "carry                     1                                             \n",
       "catch                     2                                             \n",
       "clap                      1                                             \n",
       "come                      5                 3                           \n",
       "condescend                1                                             \n",
       "confess                   1                                             \n",
       "cringe                    1                                             \n",
       "cry                      14                                             \n",
       "decide                    1                                             \n",
       "determine                 1                                             \n",
       "die                       1                                             \n",
       "do                  1     5                                             \n",
       "doubt                     1                                             \n",
       "drink                     1                                             \n",
       "drop                      1                                             \n",
       "dump                      1                                             \n",
       "eat                       1                                             \n",
       "exchange                  1                                             \n",
       "exclude                                                                1\n",
       "fall            1                                                       \n",
       "feel                1     5                                             \n",
       "...           ...  ..   ...  ...    ...   ...         ...            ...\n",
       "seem                1     1                                             \n",
       "shake                     1                                             \n",
       "shout                     4                                             \n",
       "sin                       1                                             \n",
       "sit             1                                                       \n",
       "sniff                     1                                             \n",
       "sow                       1                                             \n",
       "spring                    1                                             \n",
       "stand                     1           1                                 \n",
       "stay                2     1                                            1\n",
       "suggest                   2                                             \n",
       "take            1   1     3                                             \n",
       "teach           1   1     1                                             \n",
       "tear                      1                                             \n",
       "tell            1         4           2                 1               \n",
       "think                     2                                             \n",
       "thump                     1                                             \n",
       "tramp                     1                                             \n",
       "try                       2                                             \n",
       "understand          1                                                   \n",
       "unfasten                  1                                             \n",
       "use                       1                                             \n",
       "wait                      1                                             \n",
       "wake                      1                                             \n",
       "wheel                     1                                             \n",
       "whisper             1     1                                             \n",
       "will                      1                                             \n",
       "wound           2                                                       \n",
       "wring                     1                                             \n",
       "write                     1                                             \n",
       "\n",
       "[121 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he</td>\n",
       "      <td>grew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>her</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he</td>\n",
       "      <td>cried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>he</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>he</td>\n",
       "      <td>sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>he</td>\n",
       "      <td>loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>he</td>\n",
       "      <td>picked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>she</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>he</td>\n",
       "      <td>saved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>he</td>\n",
       "      <td>see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>he</td>\n",
       "      <td>looked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>she</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>he</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>he</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>she</td>\n",
       "      <td>looked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>she</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>he</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>she</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>her</td>\n",
       "      <td>kissed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>he</td>\n",
       "      <td>understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>he</td>\n",
       "      <td>understood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>he</td>\n",
       "      <td>advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>he</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>he</td>\n",
       "      <td>believed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>he</td>\n",
       "      <td>heard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>he</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>he</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>he</td>\n",
       "      <td>delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>he</td>\n",
       "      <td>declared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>he</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>he</td>\n",
       "      <td>engage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>he</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>he</td>\n",
       "      <td>mistook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>he</td>\n",
       "      <td>shewed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>he</td>\n",
       "      <td>thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>he</td>\n",
       "      <td>lose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>he</td>\n",
       "      <td>perform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>he</td>\n",
       "      <td>did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>he</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>he</td>\n",
       "      <td>resolved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>he</td>\n",
       "      <td>correspond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>he</td>\n",
       "      <td>give</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3762 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1\n",
       "0      he        grew\n",
       "1     her          to\n",
       "2     she        used\n",
       "3     she        made\n",
       "4      he         was\n",
       "5      he         had\n",
       "6      he        help\n",
       "7      he        went\n",
       "8      he       cried\n",
       "9      he         had\n",
       "10     he        said\n",
       "11     he         had\n",
       "12     he          be\n",
       "13     he         sat\n",
       "14     he       loved\n",
       "15     he         had\n",
       "16     he      picked\n",
       "17    she        make\n",
       "18     he       saved\n",
       "19     he         see\n",
       "20     he      looked\n",
       "21    she         was\n",
       "22     he         was\n",
       "23     he        used\n",
       "24     he          's\n",
       "25    she      looked\n",
       "26    she        said\n",
       "27     he          's\n",
       "28    she        said\n",
       "29    her      kissed\n",
       "...   ...         ...\n",
       "3732   he  understand\n",
       "3733   he        said\n",
       "3734   he  understood\n",
       "3735   he    advanced\n",
       "3736   he        went\n",
       "3737   he        said\n",
       "3738   he    believed\n",
       "3739   he       heard\n",
       "3740   he         was\n",
       "3741   he        made\n",
       "3742   he        been\n",
       "3743   he     delayed\n",
       "3744   he         was\n",
       "3745   he    declared\n",
       "3746   he       would\n",
       "3747   he         was\n",
       "3748   he      engage\n",
       "3749   he        made\n",
       "3750   he        said\n",
       "3751   he     mistook\n",
       "3752   he         was\n",
       "3753   he      shewed\n",
       "3754   he     thought\n",
       "3755   he        lose\n",
       "3756   he     perform\n",
       "3757   he         did\n",
       "3758   he          be\n",
       "3759   he    resolved\n",
       "3760   he  correspond\n",
       "3761   he        give\n",
       "\n",
       "[3762 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Method for getting interesting contexts from a novel.\n",
    "#By taking a set of novels, relevant dependancies, number of characters to consider and the desired gender,\n",
    "#it is possible to remove n useless common verbs, analyse a particular entity (along with pronoun properties)\n",
    "#to build a main character's feature set.\n",
    "#Param:  novels\n",
    "#        dependencies   - 'subj' and 'dobj'.\n",
    "#        num_characters - amount of main characters to be considered.\n",
    "#        gender         - whether feature set consists of male or female characters.\n",
    "#Return: contexts       - A counter dictionary containing name, a feature, and its occurrences.\n",
    "def get_interesting_contexts(novels, dependencies, num_characters, gender, nounphrases):\n",
    "    \n",
    "    #Method for excluding common verbs.\n",
    "    #If an entity is associated with a verb, add this via list comprehension.\n",
    "    #Then extract the most common verbs via return statement.\n",
    "    #Param:  parsed_novel.\n",
    "    #Return: list of n most common verbs.\n",
    "    def select_verbs(parsed_novel):\n",
    "        common_vrbs = [ent.text.strip().lower() for ent in parsed_novel.ents \n",
    "                       if ent.root.head.pos_ == 'VERB']\n",
    "        return [verb for verb,count in Counter(common_vrbs).most_common(n)]\n",
    "      \n",
    "    #Method for finding relevant entities.\n",
    "    #If the entity has label 'PERSON', is NOT in common verbs, has dependancies of 'subj' or 'dobj' \n",
    "    #and is of the gender we wish to extract, return true. Otherwise, return false.\n",
    "    #Param:  ent\n",
    "    #        main_characters\n",
    "    #Return: true if condition met, false otherwise.\n",
    "    def of_interest(ent,main_characters):\n",
    "        return (ent.text.strip().lower() in main_characters                         #If entity represents a main character,\n",
    "                and ent.label_ == 'PERSON'                                          #labeled as a'PERSON', \n",
    "                and ent.root.head.pos_ != common_verbs                              #NOT in common verbs,\n",
    "                and ent.root.dep_ in dependencies                                   #contains 'subj' or 'dobj' dependancies\n",
    "                and guess_gender(ent.text.strip().lower(), gender_map) == gender)   #and matchs relevant gender,return true.\n",
    "\n",
    "    contexts = defaultdict(Counter)                                              #create counter dictionary called contexts.\n",
    "    for parsed_novel in novels:                                                  #for each novel,\n",
    "        main_characters = get_main_characters(parsed_novel,num_characters)       #get their main characters,\n",
    "        common_verbs = select_verbs(parsed_novel)                                #the most common verbs and\n",
    "        get_gendered_pronoun(parsed_novel, nounphrases)                          #any verbs associated to pronouns.\n",
    "        for ent in parsed_novel.ents:                                            #For all entities in a novel,\n",
    "            if of_interest(ent,main_characters):                                 #if it's of interest,\n",
    "                contexts[ent.text.strip().lower()][ent.root.head.lemma_] += 1    #Add the name, and its feature. Increment counter.\n",
    "    return contexts\n",
    "\n",
    "nounphrases = []\n",
    "novels = {parsed_Railway, parsed_Crusoe, parsed_Romeo} #  use a set here to allow for the possibility of having multiple texts\n",
    "#novels = {parsed_Railway}\n",
    "gender = 'male'                                      #Change to either: 'male' or 'female'\n",
    "#gender = 'female'\n",
    "\n",
    "input_file = csv.DictReader(open(\"names.csv\"))         #Create gender_map.\n",
    "gender_map = create_gender_map(input_file)             \n",
    "number_of_characters_per_text = 10                     #How many main characters per novel.\n",
    "n = 100                                                #How many common verbs to remove.\n",
    "target_dependencies = {'nsubj', 'dobj'}                #Set of target dependencies.\n",
    "target_contexts = get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, gender, nounphrases)\n",
    "display(pd.DataFrame.from_dict(target_contexts).applymap(lambda x: '' if math.isnan(x) else x))\n",
    "#Dataframe for nounphrases\n",
    "display(pd.DataFrame(nounphrases))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program works as such: You provide novels to be investigated, along with the amount and gender of main characters you wish to consider. You can also select dependencies (the ones we are using are: 'subj' and 'dobj') and the popularity of verbs to consider (common verbs are transitive between characters, and so they do not provide much information on character specific features. By excluding most common verbs, we refine the search for proprietary verbs for the character). \n",
    "\n",
    "Once specified, the program will go through all novels, keeping a record of any entity that conforms to 'of_interest'. This entity is added to a dictionary of Counter called 'context'. 'context' contains a key which is a valid entity found in the novel (E.g - a main character) with a verb associated via dependency. As it is a counter, it increments when same value appears ('cry' is associated to 'Peter' 14 times).\n",
    "\n",
    "By changing some of the original parameters, like gender, we get a feature set for males across all books. This is helpful to get a visual comparison on the types of verbs, which are associated to genderised characters. For example, take 'William Atkins' from Robinson Crusoe. His feature set contains: 'call', 'behave', 'stay' and 'exclude'. Although minimal, they do suggest some sort of mischievous character: 'behave', 'exclude'. If we refer to the book, Atkins was first to turn against the captain in attempted mutiny. He was imprisoned whilst a select few went to recover the ship. Although brief and difficult to distinguish without context, the feature set created does poses relevant to defining characters features. \n",
    "\n",
    "Below I have extended the feature set by using my opinion extractor from the first assignment. I was able to provide a more specific feature set by extracting accompanying adjectives as opposed to just using the verb as the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "#Method for the opinion extractor:\n",
    "#Pass an aspect word (E.g - 'plot', 'character') and a sentence. \n",
    "#Iterate through each word in sentence, analysing the dpendancies and part of speech.\n",
    "#If words are found referencing the opinion of the review, add them to a set.\n",
    "#Param: aspect_token    - list constaining words of interest.\n",
    "#       parsed_sentence - sentence from the review which has been processed by the nlp toolkit.\n",
    "#Return: Opinions       - set containing strings describing the opinion.\n",
    "def opinion_extractor(token, opinions):  \n",
    "    \n",
    "    #opinions = []    #set to contain the opinions.\n",
    "    #opinions = token\n",
    "    #for token in parsed_sentence:    #loop to iterate through all tokens in sentence.\n",
    "    neg = \"\"\n",
    "        #if token.pos_ == 'NOUN': and token.orth_ == aspect_token:     #If token is a noun and  in aspect words, interrogate its children.\n",
    "        #    print(token.orth_)\n",
    "        #    for child in token.children:  \n",
    "        #        if child.dep_ == 'neg':     #If \"n't\" is present, add \"not-\" to beginning.\n",
    "        #            neg = \"not-\"\n",
    "        #        if child.dep_== 'amod':     #If 'amod' dependency is present, add to string.\n",
    "        #            adj_token = child.orth_\n",
    "        #            for adv_child in child.children:    #Interrogate children of the token.\n",
    "        #                if adv_child.dep_ == 'advmod':  #If child for 'advmod' dependancies in children of 'amod' token. Add to same string.\n",
    "        #                    adj_token = adv_child.orth_ + \"-\" + adj_token\n",
    "        #            opinions.append(neg + adj_token)    #append opinion to opinions.\n",
    "                \n",
    "    if token.pos_ == 'VERB':    #if token is a verb, interrogate its children. \n",
    "        \n",
    "        #print(\"verb: \" + token.orth_)\n",
    "        for child in token.children:\n",
    "            if child.pos_ == 'ADJ' or child.dep_ == 'amod':    #If an adjective is present, add to string, and interrogate its children via method: interrogate.\n",
    "                adj_token = child.orth_\n",
    "                interrogate(child, opinions)\n",
    "                     \n",
    "    return opinions\n",
    "\n",
    "#Recursive method for testing verbs and conjuctives:\n",
    "#Adds string which is either an adverbal modifier or conjuctive of a verb to the 'opinions' set.\n",
    "#If conjuctive is present, call method with conjuctive child.\n",
    "#Param: token    - word with the advmod/conj dependancy.\n",
    "#       opinions - set containing the opinions.\n",
    "def interrogate(token, opinions):\n",
    "    neg = \"\"\n",
    "    verb_token = token.orth_\n",
    "    for child in token.children:\n",
    "        if child.dep_ == 'neg':    #If \"n't\" is present, add \"not-\" to beginning.\n",
    "            neg = \"not-\"\n",
    "        if child.dep_ == 'advmod':    #If child of verb has 'advmod' dependancies.\n",
    "            verb_token = child.orth_ + \"-\" + verb_token\n",
    "        if child.dep_ == 'conj':    #If conjuctive dependency is present, call itself with current child.\n",
    "            interrogate(child, opinions)\n",
    "    \n",
    "    #print(opinions)  ->  Valid opinions\n",
    "    opinions.append(neg + verb_token)    #append opinion to opinions.\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "aspect_words = [\"characters\"]    #Aspect words to focus opinion on.   \n",
    "results = []    #set to contain results.\n",
    "\n",
    "def opinion_extractor_main(parsed_text):\n",
    "    aspect_words = [\"characters\"]    #Aspect words to focus opinion on.\n",
    "    for review in parsed_text:    #Main feature to pass sentences through nlp toolkit and the opinion extractor. \n",
    "        parsed_review = nlp(review.orth_)    #Store the review after processing it via nlp toolkit.\n",
    "        for sentence in parsed_review.sents:    #Go through every sentence in the review.\n",
    "            for aspect_token in aspect_words:    #Pass sentence through opinion extractor with aspect token.\n",
    "                opinions = opinion_extractor(aspect_token,sentence)    #Add to sets. \n",
    "                if opinions:\n",
    "                    results.append((aspect_token,sentence.orth_,opinions))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bobbie</th>\n",
       "      <th>phyllis</th>\n",
       "      <th>roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allow</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask-asleep</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask-dismayed</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-asleep</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-clever</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-eldest</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-faced</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-good</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-pale</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-right</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be-tired</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>begin</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>believe</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blow</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>break</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>break-out:--</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bring</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burn</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burst</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carry</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheer</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheer-her</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climb</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cling</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spring</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>squeeze</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stand</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroke</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suggest</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tell</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think-much</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tremble</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>try</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turn</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand-little</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wail</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wake</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warn</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wave</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whine</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whisper</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bobbie phyllis roberta\n",
       "add                    1       1        \n",
       "allow                                  1\n",
       "answer                 4                \n",
       "ask                    8       9       2\n",
       "ask-asleep                     1        \n",
       "ask-dismayed                   1        \n",
       "be                    10       6       3\n",
       "be-asleep              1                \n",
       "be-clever              1                \n",
       "be-eldest                              1\n",
       "be-faced                       1        \n",
       "be-good                        1        \n",
       "be-pale                1                \n",
       "be-right               1                \n",
       "be-tired               1                \n",
       "begin                  3       1        \n",
       "believe                        1        \n",
       "blow                   1                \n",
       "break                  2                \n",
       "break-out:--           1                \n",
       "bring                  2                \n",
       "burn                   1                \n",
       "burst                                  1\n",
       "call                   2               1\n",
       "carry                  3                \n",
       "catch                  2       2        \n",
       "cheer                          1        \n",
       "cheer-her                      1        \n",
       "climb                                  1\n",
       "cling                  2       1        \n",
       "...                  ...     ...     ...\n",
       "spring                         1        \n",
       "squeeze                1                \n",
       "stand                  1               1\n",
       "stroke                         1        \n",
       "suggest                        2        \n",
       "take                           2       1\n",
       "tell                   2                \n",
       "thank                  1                \n",
       "think                  2       1        \n",
       "think-much             1       1        \n",
       "tremble                1                \n",
       "try                    2       1        \n",
       "turn                   1       1        \n",
       "understand             2                \n",
       "understand-little      1                \n",
       "use                    1                \n",
       "wail                           1        \n",
       "wait                   3                \n",
       "wake                           2       2\n",
       "want                   2                \n",
       "warn                           1        \n",
       "watch                  1                \n",
       "wave                   1                \n",
       "when                   1                \n",
       "whine                          1        \n",
       "whisper                5       4        \n",
       "wish                   3                \n",
       "work                   1                \n",
       "would                  1                \n",
       "write                  1                \n",
       "\n",
       "[184 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he</td>\n",
       "      <td>grew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>her</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he</td>\n",
       "      <td>cried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>he</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>he</td>\n",
       "      <td>sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>he</td>\n",
       "      <td>loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>he</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>he</td>\n",
       "      <td>picked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>she</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>he</td>\n",
       "      <td>saved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>he</td>\n",
       "      <td>see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>he</td>\n",
       "      <td>looked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>she</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>he</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>he</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>she</td>\n",
       "      <td>looked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>she</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>he</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>she</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>her</td>\n",
       "      <td>kissed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>he</td>\n",
       "      <td>understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>he</td>\n",
       "      <td>understood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>he</td>\n",
       "      <td>advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>he</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>he</td>\n",
       "      <td>believed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>he</td>\n",
       "      <td>heard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>he</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>he</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>he</td>\n",
       "      <td>delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>he</td>\n",
       "      <td>declared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>he</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>he</td>\n",
       "      <td>engage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>he</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>he</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>he</td>\n",
       "      <td>mistook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>he</td>\n",
       "      <td>shewed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>he</td>\n",
       "      <td>thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>he</td>\n",
       "      <td>lose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>he</td>\n",
       "      <td>perform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>he</td>\n",
       "      <td>did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>he</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>he</td>\n",
       "      <td>resolved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>he</td>\n",
       "      <td>correspond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>he</td>\n",
       "      <td>give</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3762 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1\n",
       "0      he        grew\n",
       "1     her          to\n",
       "2     she        used\n",
       "3     she        made\n",
       "4      he         was\n",
       "5      he         had\n",
       "6      he        help\n",
       "7      he        went\n",
       "8      he       cried\n",
       "9      he         had\n",
       "10     he        said\n",
       "11     he         had\n",
       "12     he          be\n",
       "13     he         sat\n",
       "14     he       loved\n",
       "15     he         had\n",
       "16     he      picked\n",
       "17    she        make\n",
       "18     he       saved\n",
       "19     he         see\n",
       "20     he      looked\n",
       "21    she         was\n",
       "22     he         was\n",
       "23     he        used\n",
       "24     he          's\n",
       "25    she      looked\n",
       "26    she        said\n",
       "27     he          's\n",
       "28    she        said\n",
       "29    her      kissed\n",
       "...   ...         ...\n",
       "3732   he  understand\n",
       "3733   he        said\n",
       "3734   he  understood\n",
       "3735   he    advanced\n",
       "3736   he        went\n",
       "3737   he        said\n",
       "3738   he    believed\n",
       "3739   he       heard\n",
       "3740   he         was\n",
       "3741   he        made\n",
       "3742   he        been\n",
       "3743   he     delayed\n",
       "3744   he         was\n",
       "3745   he    declared\n",
       "3746   he       would\n",
       "3747   he         was\n",
       "3748   he      engage\n",
       "3749   he        made\n",
       "3750   he        said\n",
       "3751   he     mistook\n",
       "3752   he         was\n",
       "3753   he      shewed\n",
       "3754   he     thought\n",
       "3755   he        lose\n",
       "3756   he     perform\n",
       "3757   he         did\n",
       "3758   he          be\n",
       "3759   he    resolved\n",
       "3760   he  correspond\n",
       "3761   he        give\n",
       "\n",
       "[3762 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Using Opinion extractor internal\n",
    "def get_interesting_contexts(novels, dependencies, num_characters, gender, nounphrases):\n",
    "    \n",
    "    def select_verbs(parsed_novel):\n",
    "        common_vrbs = [ent.text.strip().lower() for ent in parsed_novel.ents \n",
    "                       if ent.root.head.pos_ == 'VERB']\n",
    "        return [verb for verb,count in Counter(common_vrbs).most_common(n)]\n",
    "      \n",
    "    def of_interest(ent,main_characters):\n",
    "        return (ent.text.strip().lower() in main_characters \n",
    "                and ent.label_ == 'PERSON' \n",
    "                and ent.root.head.pos_ != common_verbs\n",
    "                and ent.root.dep_ in dependencies  \n",
    "                and guess_gender(ent.text.strip().lower(), gender_map) == gender)\n",
    "\n",
    "    contexts = defaultdict(Counter)\n",
    "   # opinions = defaultdict(Counter)\n",
    "    \n",
    "    for parsed_novel in novels:\n",
    "        main_characters = get_main_characters(parsed_novel,num_characters)\n",
    "        common_verbs = select_verbs(parsed_novel)\n",
    "        opinions = []\n",
    "        get_gendered_pronoun(parsed_novel, nounphrases)                                                            \n",
    "        for ent in parsed_novel.ents:\n",
    "            if of_interest(ent,main_characters):\n",
    "                contexts[ent.text.strip().lower()][ent.root.head.lemma_] += 1\n",
    "                \n",
    "                for child in ent.root.head.children:     #If feature has associate adjective, run opinion extractor on it.\n",
    "                    if child.pos_ == 'ADJ':              \n",
    "                        #contexts[ent.text.strip().lower()][opinion_extractor(ent.root.head)] += 1\n",
    "                        #contexts[ent.text.strip().lower()][child.orth_ + \"-\" + ent.root.head.lemma_] += 1  #Add adjective to beginning.\n",
    "                        contexts[ent.text.strip().lower()][ent.root.head.lemma_ + \"-\" +  child.orth_] += 1\n",
    "                    else:\n",
    "                        adj_token = ent.root.head.lemma_                                                   #Otherwise, just the verb.\n",
    "                        \n",
    "    return contexts\n",
    "\n",
    "nounphrases = []\n",
    "novels = {parsed_Railway, parsed_Crusoe, parsed_Romeo} #  use a set here to allow for the possibility of having multiple texts\n",
    "#novels = {parsed_Railway}\n",
    "#novels = {parsed_Crusoe}\n",
    "#novels = {parsed_Romeo}\n",
    "#gender = 'male'\n",
    "gender = 'female'\n",
    "\n",
    "input_file = csv.DictReader(open(\"names.csv\"))\n",
    "gender_map = create_gender_map(input_file)             \n",
    "number_of_characters_per_text = 10\n",
    "n = 100\n",
    "target_dependencies = {'nsubj', 'dobj'}\n",
    "target_contexts = get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, gender, nounphrases)\n",
    "display(pd.DataFrame.from_dict(target_contexts).applymap(lambda x: '' if math.isnan(x) else x))\n",
    "#Dataframe for all nounphrases\n",
    "display(pd.DataFrame(nounphrases))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, I have extended the features to accommodate for ideas explored from assignment 1. The original feature sets can provide information on characters if examined thoroughly. However, by using a slightly adapted opinion extractor to identify the 'ADJ' of a given verb, we can provided a more detailed feature set, with additional adjectives. \n",
    "\n",
    "I noticed that, although a sound concept, extended features usually had only 1 member. This is very useful for picking out individual character details. However, not so much when comparing large groups of characters like genders. The concatenated features I have extracted seem be descriptive ('white-lie', 'make-tidy'). When changing the order so that adjective came first, followed by the verb, results started to change. More features became concatenated. Some less useful, but a few had the form 'be-...’ I interpreted this as useful, as it distinguished the feature was being used to give clarify on the actual individual. 'Clever' alone can be interpreted as a desire to be clever, but not actually having the quality. To 'be-clever' is to say someone is something. 'Think-much' and 'understand-little' are powerful in their presence. 'Understand' is not helpful, subtly insisting an understanding is present. As we can see, 'understand-little' is the correct context and means the opposite to understanding someone (clearly).  \n",
    "\n",
    "I have also included a pronoun extractor. By questioning whether a noun chunk phrase contains words like 'he', 'she', 'his', 'hers' you can take the root verb from the noun chunk and accompany it with its pronoun. This makes a list tuple. You can get all pronouns, which are male and female using list iteration, or methods provided in 'get_gendered_pronoun'. These can then be compared on their cosine similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have aggregated feature sets of males and females by using my gender extractor. By taking a name in a novel and comparing it to names provided in gender map, I was able to extract the gender of a main character. Not only that, I was able to pick out the forename, genderise it, and associate the surname to it (until the next title/forename has changed, and so will the surname’s gender).\n",
    "\n",
    "I was able to find roots of the entity/main character, which corresponded to a verb, which is used to describe a feature of a character. By making sure that entities of interest are aggregated via gender, I made a condition to only add entities with a select gender attached. This meant the graphs representing the feature sets were constrained by the amount of main characters to use. This was dependant on the novels used. \n",
    "\n",
    "When testing novel set: 'The Railway Children', 'Robinson Crusoe' and 'Romeo and Juliet', I found only 3 female main characters would show. This was because the only females selected were from novel 'The Railway Children'. I chose 'Romeo and Juliet' because, even though male dominant, there was one specific female who is characteristics I knew fairly well. However, Juliet did not appear as a main character. On further analysis of the corpus reader, I realised it only contained small chunks of the story; not the whole book. I believe this is a reason for male bias results. \n",
    "\n",
    "There was another interesting result when testing 'parsed_Crusoe' alone. Although spaCy successfully named x main characters in topic 8, When I set num_of_characters_per_text to 10, only 3 names returned: 'will', 'will atkins' and 'william atkins'. The most interesting about this anomaly is that the feature sets for each were different. 'atkins' produced: 'propose', 'eager-sit', 'true-say', 'teach', 'tell',  'add', etc. These are all features of some sort of leader/guide of important knowledge. However, 'william atkins' produced: 'stay', 'behave', 'call', 'exclude' and 'retire'. These are features quite the opposite. I believe this could be down to how the author has chosen to portray Will Atkins as he changes throughout the book. In summary, Will Atkins was on board a ship with Robinson Crusoe, then calls for a mutiny, and is locked away. Maybe coincidence, but from my vague knowledge of the book, these three features could well be a result of Daniel Defoe’s portrayal of characters throughout a novel. Although speculated in this instance, it is a very valid consideration. In order to generate reliable feature sets, there has to be a consideration on how a character changes throughout the book.  \n",
    "\n",
    "Below, I have tested the feature sets against each other using cosine similarity.\n",
    "\n",
    "The first method creates Counter A with all female pronouns found across all novels. The counter contains their collective feature sets. B is the same for male. A cosine similarity test taken place on the collective between male and female characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 2, 2, 2, 15, 350, 20, 9, 4, 4, 2, 2, 5, 21, 13, 2, 6, 4, 2, 13, 2, 2, 8, 3, 3, 2, 2, 2, 2, 1, 9, 3, 3, 2, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 3, 3, 1, 2, 2, 4, 2, 1, 3, 3, 4, 3, 3, 1, 7, 2, 1, 9, 3, 1, 2, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 6, 3, 1, 1, 3, 1, 1, 2, 2, 1, 1, 2, 1, 4, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[38, 0, 1, 1, 17, 423, 28, 5, 5, 2, 1, 0, 3, 21, 8, 0, 1, 6, 0, 14, 0, 0, 11, 1, 3, 0, 0, 0, 3, 1, 16, 2, 1, 3, 1, 1, 2, 4, 1, 1, 2, 1, 1, 1, 4, 2, 2, 3, 2, 6, 2, 2, 2, 4, 6, 4, 1, 1, 9, 10, 2, 7, 5, 3, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 13, 3, 1, 2, 4, 1, 1, 2, 7, 1, 1, 1, 3, 4, 3, 2, 2, 1, 5, 6, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9976894662973762"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAY HAVE TO RUN CELL BELOW BEFORE THIS ONE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#For all females in the Dictionary Counter, Concatenate all features into Counter A. Same for males below.\n",
    "for female in Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases)):\n",
    "    A += Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases))[female]\n",
    "for male in Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases)):\n",
    "    B += Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases))[male]\n",
    "\n",
    "#Method for applying cosine similarity.\n",
    "#Takes 2 counters combine to form outer set with none-transitive absent values being 0. \n",
    "#(E.g - [A:1, B:2, C:3, E:3] and [C:3,D:4,E:0] =[A:1, B:2, C:3, D:0, E:3] and [A:0, B:0, C:3, D:4, E:0]).\n",
    "#Param:  counter1\n",
    "#        counter2\n",
    "#Return: list1, list2\n",
    "def counters_to_feature_lists(counter1,counter2):\n",
    "    combined = counter1 + counter2 \n",
    "    list1 = [counter1[key] for key in combined]\n",
    "    list2 = [counter2[key] for key in combined]\n",
    "    return list1,list2\n",
    "\n",
    "L1,L2 = counters_to_feature_lists(A,B)\n",
    "print(L1)\n",
    "print(L2)\n",
    "cosine_similarity([L1], [L2])[0,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can create high similarities, as it is considered lots of data with high chances of similarity. However, results will always be below 1.0 as the sets are not completely the same. The results of this kind of analysis are consistent within 0.99. \n",
    "\n",
    "The second method is to compare characters individually against each other. This gives a good indication on specific character investigation. I have stratified a sample of 6 random characters, based on whether they are from the same novel/gender. The blue bar represents: female(roberta, roberta) vs female(bobbie phyllis), Orange: female(phyllis, bobbie) vs male(peter, phil) and Green: male(atkins, will atkins) vs male(jim, porter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAFeCAYAAACIBhjdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2477340e080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#A = 0\n",
    "#B = 0\n",
    "\n",
    "#Cosine similarity: specific characters. \n",
    "#Same novel: female vs female\n",
    "A = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases))['roberta']\n",
    "B = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases))['bobbie']\n",
    "\n",
    "#Same novel: female vs male\n",
    "C = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases))['phyllis']\n",
    "D = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases))['peter']\n",
    "\n",
    "#Same novel: male vs male\n",
    "E = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases))['will atkins']\n",
    "F = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases))['jim']\n",
    "\n",
    "#Different novel: female vs female\n",
    "G = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases))['roberta']\n",
    "H = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases))['phyllis']\n",
    "\n",
    "#Different novel: female vs male\n",
    "I = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'female', nounphrases))['bobbie']\n",
    "J = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases))['phil']\n",
    "\n",
    "#Different novel: male vs male\n",
    "K = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases))['atkins']\n",
    "L = Counter(get_interesting_contexts(novels, target_dependencies, number_of_characters_per_text, 'male', nounphrases))['porter']\n",
    "\n",
    "def counters_to_feature_lists(counter1,counter2):\n",
    "    combined = counter1 + counter2 \n",
    "    list1 = [counter1[key] for key in combined]\n",
    "    list2 = [counter2[key] for key in combined]\n",
    "    return list1,list2\n",
    "\n",
    "L1,L2 = counters_to_feature_lists(A,B)\n",
    "L3,L4 = counters_to_feature_lists(C,D)\n",
    "L5,L6 = counters_to_feature_lists(E,F)\n",
    "L7,L8 = counters_to_feature_lists(G,H)\n",
    "L9,L10 = counters_to_feature_lists(I,J)\n",
    "L11, L12 = counters_to_feature_lists(K,L)\n",
    "#print(L1)\n",
    "#print(L2)\n",
    "L1_and_L2 = cosine_similarity([L1], [L2])[0,0]\n",
    "L3_and_L4 = cosine_similarity([L3], [L4])[0,0]\n",
    "L5_and_L6 = cosine_similarity([L5], [L6])[0,0]\n",
    "L7_and_L8 = cosine_similarity([L7], [L8])[0,0]\n",
    "L9_and_L10 = cosine_similarity([L9], [L10])[0,0]\n",
    "L11_and_L12 = cosine_similarity([L11], [L12])[0,0]\n",
    "\n",
    "#list_for_similarities = [L1_and_L2, L3_and_L4, L5_and_L6, L7_and_L8, L9_and_L10, L11_and_L12] \n",
    "list_for_similarities = [('Same novel', L1_and_L2, L3_and_L4, L5_and_L6), \n",
    "                         ('Different novels', L7_and_L8, L9_and_L10, L11_and_L12)]\n",
    "\n",
    "df = pd.DataFrame(list_for_similarities)    #Print graphical representation.\n",
    "ax = df.plot.bar(title=\"Experimental Results\",legend=False,x=0)\n",
    "ax.set_ylabel(\"Similarity\")\n",
    "ax.set_xlabel(\"scale\")\n",
    "ax.set_ylim(0.3,1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, we can see there is definitely an issue with unbalanced gender sets. For reasons such as incomplete books or lack of reference (a main character is not always referenced by name) and choice of books (In particular 'The Railway Children' and 'Romeo and Juliet') my distribution of male and female characters is male bias. It seems most prosperous novel was 'The Railway Children', while 'Romeo and Juliet' did not provide Juliet as a main character unless characters_per_text was set to above 30 (This then cause problems with the cosine test). This would explain the anomalous 0.45 result for 'different novel: female vs female'.\n",
    "\n",
    "Lowest results correlate to using different novels, especially when comparing a male to a male and female to male (female to female produced the best results regardless of environment). This would suggest that male characters have more varying features between themselves than compared to a female. This does not seem entirely correct, and would most likely be down to unbalanced distribution of males to females. There were significantly more males being mentioning in the novels, however, the cosine test is only comparing two characters. Logically, this should only have a significant effect in the first method of a collective comparison. A further possibility would involve defected features extracted. The contrast between varied novelist’s attempts to portray a character can vary dramatically in the style. Style is also influenced through time. I have tried to use a span of books from 1597(Romeo and Juliet) to 1971(The Railway Children). Juliet was very badly classified, and so was Romeo. However, all main characters from 'The Railway Children' provided strong feature sets. This would suggest methods of feature extraction like this are best tailored for books published at a similar time.\n",
    "\n",
    "The blue bar for both same and different weighted the same. This can only be explained by limited female characters, most coming from 'The Railway Children'. This further confirms that characters have a higher transitivity in features when belonging to the same novel. However, this correlation hasn’t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gendered_pronoun(parsed_novel, nounphrases, nounphrases_female, nounphrases_male):\n",
    "    \n",
    "    #Method for extracting female pronoun features.\n",
    "    def gendered_pronoun_female(np):\n",
    "        return np.text.strip() in [\"she\", \"her\"]    \n",
    "    #Method for extracting male pronoun features.\n",
    "    def gendered_pronoun_male(np):\n",
    "        return np.text.strip() in [\"he\", \"his\"]\n",
    "    #Method for getting nounphrase from noun chunks within parsed novel.\n",
    "    #Param:  np.\n",
    "    #return: boolean. \n",
    "    def gendered_pronoun(np):\n",
    "        return np.text.strip() in [\"he\", \"she\", \"her\", \"his\"]\n",
    "\n",
    "    text = parsed_novel\n",
    "    \n",
    "    nounphrases_male += [[re.sub(\"\\s+\",\" \",np.text), np.root.head.text] for np in parsed_novel.noun_chunks if gendered_pronoun_male(np)]    \n",
    "    nounphrases_female += [[re.sub(\"\\s+\",\" \",np.text), np.root.head.text] for np in parsed_novel.noun_chunks if gendered_pronoun_female(np)]\n",
    "    return nounphrases_male, nounphrases_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-304acf5f846e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnounphrases_female_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_gendered_pronoun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_Railway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnounphrases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnounphrases_female\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnounphrases_male\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#Counter(get_gendered_pronoun(parsed_Railway, nounphrases, nounphrases_female, nounphrases_male))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nounphrases = []\n",
    "nounphrases_male = []\n",
    "nounphrases_male_count = defaultdict(Counter)\n",
    "nounphrases_female = []\n",
    "nounphrases_female_count = defaultdict(Counter)\n",
    "\n",
    "A, B = Counter(get_gendered_pronoun(parsed_Railway, nounphrases, nounphrases_female, nounphrases_male))\n",
    "#Counter(get_gendered_pronoun(parsed_Railway, nounphrases, nounphrases_female, nounphrases_male))\n",
    "\n",
    "for pronoun, verb in A:\n",
    "    nounphrases_male_count[pronoun][verb] += 1\n",
    "for pronoun, verb in B:\n",
    "    nounphrases_female_count[pronoun][verb] += 1\n",
    "print(nounphrases_female_count)\n",
    "#A = nounphrases_male_count[pronoun]['he']\n",
    "#B = nounphrases_male_count[pronoun]['he']\n",
    "\n",
    "def counters_to_feature_lists(counter1,counter2):\n",
    "    combined = counter1 + counter2 \n",
    "    list1 = [counter1[key] for key in combined]\n",
    "    list2 = [counter2[key] for key in combined]\n",
    "    return list1,list2\n",
    "\n",
    "L1,L2 = counters_to_feature_lists(A,B)\n",
    "print(L1)\n",
    "print(L2)\n",
    "cosine_similarity([L1], [L2])[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
